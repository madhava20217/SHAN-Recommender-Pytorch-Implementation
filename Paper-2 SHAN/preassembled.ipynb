{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import srdatasets\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchmetrics import functional as tm_f\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device =torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "srdatasets process --dataset=Amazon-VideoGames --split-by=user --task=long-short --target-len=1 --session-interval=120 --min-freq-item=0 --min-freq-user=0\n",
    "\n",
    "srdatasets process --dataset=Amazon-Books --split-by=time --task=long-short --input-len=5 --target-len=10 --pick-target=last --session-interval=14400 --min-freq-item=0 --min-freq-user=0 --min-session-len=11 --pre-sessions=6\n",
    "\n",
    "srdatasets process --dataset=Amazon-VideoGames --split-by=user --test-split=0.2 --dev-split=0.1 --task=long-short --input-len=5 --target-len=10 --pre-sessions=10 --pick-targets=last --session-interval=14400 --min-session-len=11 --max-session-len=30\n",
    ": c1683467389709\n",
    "\n",
    "srdatasets process --dataset=Gowalla --split-by=user --test-split=0.2 --dev-split=0.1 --task=long-short --input-len=5 --target-len=1 --pre-sessions=10 --pick-targets=last --session-interval=1440 --min-session-len=2 --max-session-len=30 --min-freq-item=20 --min-freq-user=20\n",
    ": c1683467693343\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217, 7, 31)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from srdatasets.dataloader_pytorch import DataLoader\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "DATASET_CODE_VIDEOGAME = 'c1683466871546'\n",
    "DATASET_CODE_BOOKS = 'c1683461837235'\n",
    "DATASET_CODE_GOWALLA = 'c1683468658591'\n",
    "\n",
    "\n",
    "DATASET_CODE = DATASET_CODE_GOWALLA\n",
    "DATASET = \"Gowalla\"\n",
    "\n",
    "trainloader = DataLoader(DATASET, DATASET_CODE, batch_size=BATCH_SIZE, train=True, negatives_per_target=5, include_timestamp=True)#, num_workers=8, pin_memory=True)\n",
    "valloader = DataLoader(DATASET, DATASET_CODE, batch_size=BATCH_SIZE, train=False, development = True, include_timestamp=True)#, num_workers=8, pin_memory=True)\n",
    "testloader = DataLoader(DATASET, DATASET_CODE, batch_size=BATCH_SIZE, train=False, development = False, include_timestamp=True)#, num_workers=8, pin_memory=True)\n",
    "\n",
    "len(trainloader), len(valloader), len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9908, 41012)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users, n_items = trainloader.num_users, trainloader.num_items\n",
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10808, 349, 1525)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_train = len(trainloader.dataset)\n",
    "len_val = len(valloader.dataset)\n",
    "len_test = len(testloader.dataset)\n",
    "\n",
    "len_train, len_val, len_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHAN(nn.Module):\n",
    "    def __init__(self, embedding_dims, n_users = n_users, n_items = n_items):\n",
    "        super().__init__()\n",
    "\n",
    "        #user\n",
    "        self.user_embed = nn.Embedding(n_users, embedding_dims)\n",
    "\n",
    "        #item\n",
    "        self.item_embed = nn.Embedding(n_items, embedding_dims, 0)\n",
    "\n",
    "        #long-term layer\n",
    "        self.item_trans1 = nn.Linear(embedding_dims, embedding_dims)\n",
    "        self.act_1 = nn.ReLU()\n",
    "\n",
    "        #after taking the embeddings of the item (v), feedforward them through the item_tran1 network to get h\n",
    "        #thereafter, compute the attention weights of each item by taking the softmax activation of the dotted user.h\n",
    "        #compute u_long by taking attention_weights * v for each\n",
    "\n",
    "        #short-term layer\n",
    "        self.item_trans2 = nn.Linear(embedding_dims, embedding_dims)\n",
    "        self.act_2 = nn.ReLU()\n",
    "\n",
    "        #weighting for net user representation\n",
    "        self.beta_0 = torch.randn(1, requires_grad = True).to(device)\n",
    "    \n",
    "    def forward(self, users, pre_sessions_items, cur_session_items):\n",
    "        user_rep = self.user_embed(users)[..., None]                            # batch * emb * 1\n",
    "        \n",
    "        \n",
    "        # LONG TERM \n",
    "        long_term_item_rep = self.item_embed(pre_sessions_items)\n",
    "        activated_long_term = self.act_1(self.item_trans1(long_term_item_rep))  # batch * num * emb\n",
    "        #compute attention weights\n",
    "        attention_mat_1 = torch.bmm(activated_long_term, user_rep)              # batch * num * 1     \n",
    "        attention_weights_1 = F.softmax(attention_mat_1, dim = 1)               # batch * num * 1\n",
    "        #long term representation\n",
    "        u_long = attention_weights_1*long_term_item_rep                         # batch * num * emb\n",
    "        u_long = torch.sum(u_long, dim = 1)                                     # batch * emb\n",
    "\n",
    "        #SHORT TERM\n",
    "        short_term_item_rep = self.item_embed(cur_session_items)\n",
    "        activated_short_term = self.act_2(self.item_trans2(short_term_item_rep))\n",
    "        #compute attention weights \n",
    "        attention_mat_2 = torch.bmm(activated_short_term, user_rep)\n",
    "        attention_weights_2 = F.softmax(attention_mat_2, dim = 1)\n",
    "        #short term representation\n",
    "        u_short = attention_weights_2*short_term_item_rep\n",
    "        u_short = torch.sum(u_short, dim = 1)\n",
    "\n",
    "        # HYBRID\n",
    "        u_hybrid = self.beta_0*u_long + u_short\n",
    "\n",
    "        preference_scores = u_hybrid @ self.item_embed.weight.T # batch * emb @ (n_items x emb).T\n",
    "        return preference_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, target, bootstraps = 100):\n",
    "    bs, nitms = preds.size()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i in range(bootstraps):\n",
    "        idx = torch.randint(0, nitms, (bs, 1)).to(device)\n",
    "        others = preds.gather(1, idx)\n",
    "        actual = preds.gather(1, target)\n",
    "\n",
    "        loss = -F.logsigmoid(actual - others)\n",
    "        loss = torch.mean(loss)\n",
    "        total_loss+= loss\n",
    "\n",
    "    return total_loss/bootstraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:02<00:00, 82.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(9907), tensor(41052))"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = 0\n",
    "n_items = 0\n",
    "for users, pre_sessions_items, cur_session_items, target_items, _, _, _, _ in tqdm(iter(trainloader)):\n",
    "    n_users = max(n_users, max(users))\n",
    "    n_items = max(n_items, pre_sessions_items.max())\n",
    "    n_items = max(n_items, cur_session_items.max())\n",
    "    n_items = max(n_items, target_items.max())\n",
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_function(func, preds, targets, k = 10):\n",
    "    b = torch.zeros(preds.shape).bool()\n",
    "    b[targets] = True\n",
    "    b = b.to(device)\n",
    "    \n",
    "    return func(preds, b, k = k)\n",
    "\n",
    "def get_batch_func(func, preds, targets, k=10, averaging = None):\n",
    "    val = []\n",
    "    for i in range(len(preds)):\n",
    "        val.append(get_function(func, preds[i], targets[i], k).item())\n",
    "\n",
    "    if(averaging is None):\n",
    "        return np.sum(val)\n",
    "    return np.mean(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 37.16it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 98.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 1.371490).  Saving model ...\n",
      "Epoch 1: Training loss: 1.7037, Validation loss: 1.3715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 38.88it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 92.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.371490 --> 1.069517).  Saving model ...\n",
      "Epoch 2: Training loss: 1.1322, Validation loss: 1.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 38.55it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 101.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.069517 --> 0.874238).  Saving model ...\n",
      "Epoch 3: Training loss: 0.8459, Validation loss: 0.8742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 39.17it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 83.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.874238 --> 0.800087).  Saving model ...\n",
      "Epoch 4: Training loss: 0.7234, Validation loss: 0.8001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 38.66it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 87.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.800087 --> 0.781565).  Saving model ...\n",
      "Epoch 5: Training loss: 0.6670, Validation loss: 0.7816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 38.41it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 89.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.781565 --> 0.770248).  Saving model ...\n",
      "Epoch 6: Training loss: 0.6318, Validation loss: 0.7702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 38.34it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 96.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.770248 --> 0.755334).  Saving model ...\n",
      "Epoch 7: Training loss: 0.6027, Validation loss: 0.7553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 38.01it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 99.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.755334 --> 0.745823).  Saving model ...\n",
      "Epoch 8: Training loss: 0.5749, Validation loss: 0.7458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 38.40it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 87.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch 9: Training loss: 0.5519, Validation loss: 0.7685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 39.25it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 102.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 3\n",
      "Epoch 10: Training loss: 0.5312, Validation loss: 0.7617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:05<00:00, 39.55it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 94.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DIMS = 50\n",
    "\n",
    "model = SHAN(embedding_dims= DIMS, n_users = n_users +1, n_items = n_items+1).to(device)\n",
    "early_stopping = EarlyStopping(patience=3, verbose=True, path = 'shan_ed_{}.pth'.format(DIMS))\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "NUM_EPOCHS = 20\n",
    "BOOTSTRAPS = 20\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# train_prec_5 = []\n",
    "# train_rec_5 = []\n",
    "# train_prec_1 = []\n",
    "# train_rec_1 = []\n",
    "# train_prec_10 = []\n",
    "# train_rec_10 = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "\n",
    "    # Train\n",
    "    for users, pre_sessions_items, cur_session_items, target_items, _, _, _, _ in tqdm(iter(trainloader)):\n",
    "        # Shape\n",
    "        #   users:                          (batch_size,)\n",
    "        #   pre_sessions_items:             (batch_size, pre_sessions * max_session_len)\n",
    "        #   cur_session_items:              (batch_size, max_session_len - target_len)\n",
    "        #   target_items:                   (batch_size, target_len)\n",
    "        #   negative_samples:               (batch_size, target_len, negatives_per_target)\n",
    "        # DataType\n",
    "        #   numpy.ndarray or torch.LongTensor\\\n",
    "        optim.zero_grad()\n",
    "        users = users.to(device)\n",
    "        pre_sessions_items = pre_sessions_items.to(device)\n",
    "        cur_session_items = cur_session_items.to(device)\n",
    "        target_items = target_items.to(device)\n",
    "\n",
    "        preferences = model(users, pre_sessions_items, cur_session_items)\n",
    "        loss = loss_fn(preferences, target_items, BOOTSTRAPS)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        net_loss+=loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    net_loss_val = 0\n",
    "    # Val\n",
    "    for users, pre_sessions_items, cur_session_items, target_items, _, _, _ in tqdm(iter(valloader)):\n",
    "        with torch.no_grad():\n",
    "            users = users.to(device)\n",
    "            pre_sessions_items = pre_sessions_items.to(device)\n",
    "            cur_session_items = cur_session_items.to(device)\n",
    "            target_items = target_items.to(device)\n",
    "\n",
    "            preferences = model(users, pre_sessions_items, cur_session_items)\n",
    "            loss = loss_fn(preferences, target_items, BOOTSTRAPS)\n",
    "\n",
    "            net_loss_val+=loss.item()\n",
    "    net_loss = net_loss/len(trainloader)\n",
    "    net_loss_val = net_loss_val/len(valloader)\n",
    "    early_stopping(net_loss_val, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        print('-'*60)\n",
    "        break\n",
    "\n",
    "    print(\"Epoch {}: Training loss: {:.4f}, Validation loss: {:.4f}\".format(epoch+1, net_loss, net_loss_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shan_ed_{}.pth'.format(DIMS), 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2000, device='cuda:0')"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_function(tm_f.retrieval_recall, preferences[0], target_items[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:06<00:00,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1 = 0.42950819672131146, @5 = 0.11921311659891097, @10 = 0.06504918139000408\n",
      "Recall@1 = 0.08518605391510198, @5 = 0.11749518792648785, @10 = 0.12675123802951124\n",
      "0.7085349559783936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net_loss_test = 0\n",
    "\n",
    "\n",
    "net_loss = 0\n",
    "prec_5_epoch = 0\n",
    "rec_5_epoch = 0\n",
    "prec_1_epoch = 0\n",
    "rec_1_epoch = 0\n",
    "prec_10_epoch = 0\n",
    "rec_10_epoch = 0\n",
    "# Test\n",
    "for users, pre_sessions_items, cur_session_items, target_items, _, _, _ in tqdm(iter(testloader)):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        \n",
    "        users = users.to(device)\n",
    "        pre_sessions_items = pre_sessions_items.to(device)\n",
    "        cur_session_items = cur_session_items.to(device)\n",
    "        target_items = target_items.to(device)\n",
    "\n",
    "        preferences = model(users, pre_sessions_items, cur_session_items)\n",
    "        loss_fn(preferences, target_items, BOOTSTRAPS)\n",
    "\n",
    "        net_loss_test+=loss.item()\n",
    "        \n",
    "        \n",
    "        prec_5_epoch+= get_batch_func(tm_f.retrieval_precision, preferences, target_items, 5, averaging = None)\n",
    "        rec_5_epoch+= get_batch_func(tm_f.retrieval_recall, preferences, target_items, 5, averaging = None)\n",
    "        prec_1_epoch+= get_batch_func(tm_f.retrieval_precision, preferences, target_items, 1, averaging = None)\n",
    "        rec_1_epoch+= get_batch_func(tm_f.retrieval_recall, preferences, target_items, 1, averaging = None)\n",
    "        prec_10_epoch+= get_batch_func(tm_f.retrieval_precision, preferences, target_items, 10, averaging = None)\n",
    "        rec_10_epoch+= get_batch_func(tm_f.retrieval_recall, preferences, target_items, 10, averaging = None)\n",
    "        \n",
    "\n",
    "prec_10_epoch/=len_test\n",
    "prec_5_epoch/=len_test\n",
    "prec_1_epoch/=len_test\n",
    "rec_10_epoch/=len_test\n",
    "rec_5_epoch/=len_test\n",
    "rec_1_epoch/=len_test\n",
    "\n",
    "        # train_prec_5.append(prec_5_epoch)\n",
    "        # train_prec_1.append(prec_1_epoch)\n",
    "        # train_prec_10.append(prec_10_epoch)\n",
    "\n",
    "        # train_rec_5.append(rec_5_epoch)\n",
    "        # train_rec_1.append(rec_1_epoch)\n",
    "        # train_rec_10.append(rec_10_epoch)\n",
    "\n",
    "print(\"Precision@1 = {}, @5 = {}, @10 = {}\".format(prec_1_epoch, prec_5_epoch, prec_10_epoch))\n",
    "print(\"Recall@1 = {}, @5 = {}, @10 = {}\".format(rec_1_epoch, rec_5_epoch, rec_10_epoch))\n",
    "net_loss_test = net_loss_test/len(testloader)\n",
    "\n",
    "print(net_loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 41053])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[40525, 34039, 34039, 40524, 40525, 40525, 40524, 34039, 40526, 40525],\n",
       "        [40528, 32767, 40527, 28788, 40528, 37824, 40527, 37824, 28788, 40528],\n",
       "        [ 9066, 15051, 40549, 12786, 40549,  3148, 14566,  3897,  9074, 14571],\n",
       "        [ 6019, 26255, 26531, 23989, 34764, 34767, 27766, 36897, 36897, 27766],\n",
       "        [40593, 35318, 40593, 40593, 40592, 40592, 35325, 40592, 35325, 40593],\n",
       "        [40592, 35318, 40592, 35318, 40592, 35318, 40592, 40593, 35318, 35320],\n",
       "        [40600, 40601, 40602, 40601, 40597, 40599, 40602, 37844, 40600, 37844],\n",
       "        [28546, 10633, 20985, 28546, 10633, 13451, 40675, 13451, 28546, 23043],\n",
       "        [32980, 28873, 35983,  6055, 25244,  3287, 40677, 13335, 35983, 32980],\n",
       "        [40678, 40679, 40682, 23786, 29511, 40678, 25263, 35983, 14630, 40682],\n",
       "        [36094, 35582, 35581, 40693, 35580, 40693, 35578, 36094, 35580, 35581],\n",
       "        [18143, 16449, 16446,  7137, 35645, 15435, 15434,  7137,  7138, 28037],\n",
       "        [15850,  3217, 40733, 10451, 15850, 15850,  3217, 40732, 40733, 40733],\n",
       "        [40738, 40740, 40740, 40740, 40738, 40738, 40740, 40740, 40738, 40740],\n",
       "        [23953, 28889, 35901, 40923, 20790, 40923, 20770,  6017,  6017, 20770],\n",
       "        [40933, 30327, 40357, 40933, 40357, 18934, 29266, 40933, 33856, 35225],\n",
       "        [14523, 40937, 31121, 27668, 40937, 31121, 40937, 31121, 31121,  3217],\n",
       "        [20860, 20391,  3263, 40939,  4515, 20859, 20861, 40944,  4523, 40941],\n",
       "        [ 1106,  1120,  1109,  1118,  1115,  1110,  1125,  1112,   704,    27],\n",
       "        [39052, 39053, 39057, 38714, 39052, 39053, 27513, 39052, 39053,    83],\n",
       "        [34476, 22606, 34488, 22606, 40962, 34476, 34477, 31167, 34488, 22606],\n",
       "        [ 2197,  5993,  1697, 11680, 11679,  9687,  2197, 19974, 20061, 40973],\n",
       "        [38390, 35974, 40995, 35279, 35627, 40995,  8916, 35867, 40995, 38610],\n",
       "        [25218, 22897, 41006, 41006, 41004, 41005, 41006, 25263, 11506, 41006],\n",
       "        [20404, 34079, 34082, 34081, 34376, 34079, 27639, 20404, 34079, 34082]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0675e-02, -2.5502e+00, -7.5968e-02,  ..., -2.9945e+00,\n",
       "         -1.5199e-01, -5.5077e-01],\n",
       "        [ 4.0324e-02, -4.4944e-01,  1.3139e-01,  ...,  1.2590e+00,\n",
       "          5.7969e-01,  3.9998e-01],\n",
       "        [ 1.0541e-02, -1.0599e+00, -7.9814e-01,  ...,  1.8736e-01,\n",
       "          2.3158e+00,  6.9104e-01],\n",
       "        ...,\n",
       "        [-3.0522e-02,  9.9574e-01,  1.5569e-01,  ...,  3.5698e-03,\n",
       "         -1.4902e+00, -1.7064e+00],\n",
       "        [ 2.6391e-02,  2.3572e+00, -7.8037e-01,  ...,  4.2007e+00,\n",
       "          6.0871e+00,  1.9057e+00],\n",
       "        [-9.6658e-03, -3.4209e-01,  5.9871e+00,  ..., -1.9429e+00,\n",
       "         -1.0512e+00,  1.1403e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[40525, 40524, 11283,  6200, 11399, 29969, 15215,  4039,  3120, 17432],\n",
       "        [40527, 38350, 40595,  5572, 27551, 21674, 25692,  1789, 31363, 26545],\n",
       "        [40549, 26084, 40944, 26146,  5266, 34855,   681, 36688, 14571,  7189],\n",
       "        [21189, 22284, 34746, 37933, 25531, 11962, 17992, 35475, 11675, 29253],\n",
       "        [40593, 35318, 10191,  4058,  5131, 23654, 22580, 40592, 14345, 39813],\n",
       "        [35318, 23654, 11671, 40593, 38518, 23975, 27562, 22580,  8977, 36135],\n",
       "        [24176, 37844, 10940,  1020, 33201, 40597, 29431, 32216, 29009, 33319],\n",
       "        [13451, 35617,  4838, 33653, 15221, 29558, 37328, 17555,  1553, 32149],\n",
       "        [32980, 31378,  7060,  6836,  5801, 28460, 23787, 34825, 10474, 26081],\n",
       "        [40679, 28879, 33764,  5371, 40850, 28006, 35104, 13698, 22249, 16810],\n",
       "        [35579, 22673, 26937, 12693, 24683, 13958, 12391,  1925, 33315,  4836],\n",
       "        [16450, 40702, 34389, 35772,  2348, 37688, 27146, 17365, 34439,  2093],\n",
       "        [15850,  8154,  2255, 29204, 23870, 34308, 18057,  6891, 37875, 19000],\n",
       "        [40740, 12417, 32337, 40610,  8809, 38838,  9155, 31824, 16677, 14897],\n",
       "        [10485, 15614, 25998,  5104,  3839, 23431, 31531, 38382, 20136,  6924],\n",
       "        [40933, 10838, 40625, 10149, 38736, 29995, 22030,  1955,  7411,  9758],\n",
       "        [40937, 38905, 23554, 34371, 27540,  9621, 36890,  2188, 38979,  3504],\n",
       "        [35449,  3264, 32302, 30623, 26664, 41050, 29460,   437, 14386,  4474],\n",
       "        [ 1114,  9359, 27469,  8090, 14001, 38132, 37213, 33442, 24014,  5901],\n",
       "        [39052, 39053, 14109, 16458, 26062, 33249,  4949, 33215, 19294,  5029],\n",
       "        [34488, 18862,  7909, 35964, 10548,  4654, 40113, 11775,   519, 25224],\n",
       "        [11287, 28072, 26864,  7362,  9294, 40235, 36161, 24882, 29983, 22832],\n",
       "        [40995,  3087,  8878, 16946,  4919, 25415, 26818,  8336, 26709, 35421],\n",
       "        [41006,  8295, 10454, 38719, 19135, 29329, 24220, 14971,  3250, 40734],\n",
       "        [27639, 14597, 35919, 16949, 38378, 10363, 30765, 26235, 13516, 40396]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(preferences, k = 10, dim = -1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[40525, 34039, 34039, 40524, 40525, 40525, 40524, 34039, 40526, 40525],\n",
       "        [40528, 32767, 40527, 28788, 40528, 37824, 40527, 37824, 28788, 40528],\n",
       "        [ 9066, 15051, 40549, 12786, 40549,  3148, 14566,  3897,  9074, 14571],\n",
       "        [ 6019, 26255, 26531, 23989, 34764, 34767, 27766, 36897, 36897, 27766],\n",
       "        [40593, 35318, 40593, 40593, 40592, 40592, 35325, 40592, 35325, 40593],\n",
       "        [40592, 35318, 40592, 35318, 40592, 35318, 40592, 40593, 35318, 35320],\n",
       "        [40600, 40601, 40602, 40601, 40597, 40599, 40602, 37844, 40600, 37844],\n",
       "        [28546, 10633, 20985, 28546, 10633, 13451, 40675, 13451, 28546, 23043],\n",
       "        [32980, 28873, 35983,  6055, 25244,  3287, 40677, 13335, 35983, 32980],\n",
       "        [40678, 40679, 40682, 23786, 29511, 40678, 25263, 35983, 14630, 40682],\n",
       "        [36094, 35582, 35581, 40693, 35580, 40693, 35578, 36094, 35580, 35581],\n",
       "        [18143, 16449, 16446,  7137, 35645, 15435, 15434,  7137,  7138, 28037],\n",
       "        [15850,  3217, 40733, 10451, 15850, 15850,  3217, 40732, 40733, 40733],\n",
       "        [40738, 40740, 40740, 40740, 40738, 40738, 40740, 40740, 40738, 40740],\n",
       "        [23953, 28889, 35901, 40923, 20790, 40923, 20770,  6017,  6017, 20770],\n",
       "        [40933, 30327, 40357, 40933, 40357, 18934, 29266, 40933, 33856, 35225],\n",
       "        [14523, 40937, 31121, 27668, 40937, 31121, 40937, 31121, 31121,  3217],\n",
       "        [20860, 20391,  3263, 40939,  4515, 20859, 20861, 40944,  4523, 40941],\n",
       "        [ 1106,  1120,  1109,  1118,  1115,  1110,  1125,  1112,   704,    27],\n",
       "        [39052, 39053, 39057, 38714, 39052, 39053, 27513, 39052, 39053,    83],\n",
       "        [34476, 22606, 34488, 22606, 40962, 34476, 34477, 31167, 34488, 22606],\n",
       "        [ 2197,  5993,  1697, 11680, 11679,  9687,  2197, 19974, 20061, 40973],\n",
       "        [38390, 35974, 40995, 35279, 35627, 40995,  8916, 35867, 40995, 38610],\n",
       "        [25218, 22897, 41006, 41006, 41004, 41005, 41006, 25263, 11506, 41006],\n",
       "        [20404, 34079, 34082, 34081, 34376, 34079, 27639, 20404, 34079, 34082]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for users, pre_sessions_items, cur_session_items, target_items, pre_sessions_item_timestamps, cur_session_item_timestamps, target_item_timestamps in valloader:\n",
    "#     pass\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('torchnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae2c75324e7fdf5ebd22146e2daffaa477f8ea149f0e685be4c317c2939a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
